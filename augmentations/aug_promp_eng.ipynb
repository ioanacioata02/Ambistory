{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8d48e1c080474aaeadeb17cd0087f922",
            "b4885ed417c04110838ef7450afceff3",
            "e61872d0b4e24116b6d78741d8618b0b",
            "88d6859d500641fa9d8aab03562e1d48",
            "3d2a38830c6a4e448ee8990763ae1af7",
            "9893409049e94e06900d27a726a6d560",
            "7fd031807d344d45a204e0950f0f64f4",
            "2238096b40e64d7c9ecea8d66cf874b2",
            "8bd74585b8c641aa98d3630156470512",
            "5f17cb8b0c2d48bbbad94cae58ee6eab",
            "b0d0305c516346f0a1fe0c5bad66ad66"
          ]
        },
        "id": "xBMY8iA7DbeZ",
        "outputId": "1aed260a-8cd4-4ce1-e40f-ccf26cdad540"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "\n",
        "CONFIG = {\n",
        "    \"input_path\": \"/kaggle/input/ambistory-raw/train.json\",\n",
        "\n",
        "    \"model_name\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "\n",
        "    \"batch_size\": 4,\n",
        "    \"max_new_tokens\": 1200,\n",
        "\n",
        "    \"temperature\": 0.7,\n",
        "    \"fallback_temperature\": 1.0,\n",
        "    \"top_p\": 0.9,\n",
        "\n",
        "    \"progress_bar\": True,\n",
        "    \"slice_start\": 0,\n",
        "    \"slice_end\": 1140,\n",
        "\n",
        "    \"max_retries\": 2,\n",
        "    \"debug\": True,\n",
        "    \"debug_char_limit\": 500\n",
        "}\n",
        "\n",
        "\n",
        "def make_output_path():\n",
        "    start = CONFIG[\"slice_start\"]\n",
        "    end = CONFIG[\"slice_end\"]\n",
        "    end_str = \"end\" if end is None else str(end)\n",
        "    return f\"train_aug_slice_{start}_{end_str}.json\"\n",
        "\n",
        "\n",
        "OUTPUT_PATH = make_output_path()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"\\n===== HARDWARE DEBUG =====\")\n",
        "print(\"Model device:\", next(model.parameters()).device)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")\n",
        "print(\"Output path:\", OUTPUT_PATH)\n",
        "print(\"==========================\\n\")\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a careful linguistic annotator. \"\n",
        "    \"You strictly follow instructions and output only valid JSON.\"\n",
        ")\n",
        "\n",
        "USER_PROMPT = \"\"\"\n",
        "You are given multiple training instances from a word sense plausibility task.\n",
        "\n",
        "For EACH instance, create EXACTLY ONE augmented version.\n",
        "\n",
        "Rules:\n",
        "- Rewrite the precontext. All three sentences must be lexically and syntactically different from the original.\n",
        "- Rewrite the ambiguous sentence so it remains equally ambiguous and fully compatible with BOTH original word senses.\n",
        "- The ambiguous sentence MUST NOT contain any technical, domain-specific, or sense-specific modifiers that were not present in the original.\n",
        "- The ambiguous sentence must remain natural and valid under both interpretations, without favoring either.\n",
        "- Rewrite the judged meaning using a semantically equivalent formulation.\n",
        "- Rewrite the example sentence so it exemplifies the rewritten judged meaning to the same degree of implicitness as the original.\n",
        "- If the ending is NON-EMPTY, rewrite it with the same degree of bias toward the judged meaning (no stronger, no weaker).\n",
        "- If the ending is EMPTY, keep it EMPTY.\n",
        "- DO NOT clarify the word sense.\n",
        "- DO NOT introduce explicit sense markers.\n",
        "- DO NOT increase or decrease overall plausibility.\n",
        "- Keep each story coherent and natural.\n",
        "- Do NOT include any text outside the JSON list.\n",
        "\n",
        "Before producing the final JSON, silently verify that the ambiguous sentence still supports both original senses. If it does not, rewrite it again.\n",
        "\n",
        "Return ONLY a JSON LIST of augmented instances, in the SAME ORDER as the input.\n",
        "\n",
        "Instances:\n",
        "{instances}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def extract_first_json_list(text):\n",
        "    start = text.find(\"[\")\n",
        "    if start == -1:\n",
        "        return None\n",
        "    depth = 0\n",
        "    for i in range(start, len(text)):\n",
        "        if text[i] == \"[\":\n",
        "            depth += 1\n",
        "        elif text[i] == \"]\":\n",
        "            depth -= 1\n",
        "            if depth == 0:\n",
        "                return text[start:i + 1]\n",
        "    return None\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_once(instances_json, temperature, do_sample):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": USER_PROMPT.format(instances=instances_json)}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
        "        temperature=temperature,\n",
        "        top_p=CONFIG[\"top_p\"],\n",
        "        do_sample=do_sample,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_tokens = output[0][input_ids.shape[-1]:]\n",
        "    decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    extracted = extract_first_json_list(decoded)\n",
        "\n",
        "    # if CONFIG[\"debug\"]:\n",
        "    #     print(\"\\n===== RAW MODEL OUTPUT (TRIMMED) =====\")\n",
        "    #     print(decoded[:CONFIG[\"debug_char_limit\"]])\n",
        "    #     print(\"=====================================\\n\")\n",
        "\n",
        "    return decoded, extracted\n",
        "\n",
        "\n",
        "def validate_aug_batch(aug_batch, expected_len):\n",
        "    return (\n",
        "        isinstance(aug_batch, list)\n",
        "        and len(aug_batch) == expected_len\n",
        "        and all(isinstance(x, dict) for x in aug_batch)\n",
        "    )\n",
        "\n",
        "\n",
        "def is_modified(original, augmented):\n",
        "    fields = [\"precontext\", \"sentence\", \"judged_meaning\", \"example_sentence\"]\n",
        "    return any(\n",
        "        str(original.get(f, \"\")).strip().lower()\n",
        "        != str(augmented.get(f, \"\")).strip().lower()\n",
        "        for f in fields\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_batch_with_retry(instances_json, expected_len):\n",
        "    for attempt in range(CONFIG[\"max_retries\"]):\n",
        "        try:\n",
        "            decoded, extracted = generate_once(\n",
        "                instances_json,\n",
        "                CONFIG[\"temperature\"] if attempt == 0 else CONFIG[\"fallback_temperature\"],\n",
        "                do_sample=(attempt == 0)\n",
        "            )\n",
        "\n",
        "            if extracted is None:\n",
        "                raise ValueError(\"No JSON list found\")\n",
        "\n",
        "            aug = json.loads(extracted)\n",
        "            if validate_aug_batch(aug, expected_len):\n",
        "                return aug\n",
        "\n",
        "        except Exception as e:\n",
        "            if CONFIG[\"debug\"]:\n",
        "                print(f\"[DEBUG RETRY] {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def main():\n",
        "    with open(CONFIG[\"input_path\"], \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    keys = sorted(data.keys(), key=int)\n",
        "    start = CONFIG[\"slice_start\"]\n",
        "    end = CONFIG[\"slice_end\"] or len(keys)\n",
        "    keys = keys[start:end]\n",
        "\n",
        "    max_sample_id_global = max(int(v[\"sample_id\"]) for v in data.values())\n",
        "\n",
        "    global_offset = CONFIG[\"slice_start\"]\n",
        "    local_instance_offset = 0\n",
        "\n",
        "    augmented_data = {}\n",
        "\n",
        "    iterator = range(0, len(keys), CONFIG[\"batch_size\"])\n",
        "    if CONFIG[\"progress_bar\"]:\n",
        "        iterator = tqdm(iterator, desc=f\"Augmenting slice {start}:{end}\")\n",
        "\n",
        "    for i in iterator:\n",
        "        batch_keys = keys[i:i + CONFIG[\"batch_size\"]]\n",
        "        originals = [data[k] for k in batch_keys]\n",
        "\n",
        "        batch_for_prompt = []\n",
        "        for item in originals:\n",
        "            d = dict(item)\n",
        "            d.pop(\"sample_id\")\n",
        "            batch_for_prompt.append(d)\n",
        "\n",
        "        aug_batch = generate_batch_with_retry(\n",
        "            json.dumps(batch_for_prompt, ensure_ascii=False),\n",
        "            len(batch_for_prompt)\n",
        "        )\n",
        "\n",
        "        if aug_batch is None:\n",
        "            print(f\"[SKIP BATCH {i}] Failed\")\n",
        "            continue\n",
        "\n",
        "        for orig, aug in zip(originals, aug_batch):\n",
        "            if not is_modified(orig, aug):\n",
        "                print(\"\\n[REJECTED AS IDENTICAL]\")\n",
        "                continue\n",
        "\n",
        "            sample_id = (\n",
        "                max_sample_id_global\n",
        "                + global_offset\n",
        "                + local_instance_offset\n",
        "                + 1\n",
        "            )\n",
        "\n",
        "            aug[\"sample_id\"] = str(sample_id)\n",
        "            augmented_data[str(sample_id)] = aug\n",
        "\n",
        "            local_instance_offset += 1\n",
        "\n",
        "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(augmented_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Done. Augmented instances written: {len(augmented_data)}\")\n",
        "    print(f\"Output file: {OUTPUT_PATH}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2238096b40e64d7c9ecea8d66cf874b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d2a38830c6a4e448ee8990763ae1af7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f17cb8b0c2d48bbbad94cae58ee6eab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fd031807d344d45a204e0950f0f64f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88d6859d500641fa9d8aab03562e1d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f17cb8b0c2d48bbbad94cae58ee6eab",
            "placeholder": "​",
            "style": "IPY_MODEL_b0d0305c516346f0a1fe0c5bad66ad66",
            "value": " 2/2 [00:03&lt;00:00,  1.45s/it]"
          }
        },
        "8bd74585b8c641aa98d3630156470512": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d48e1c080474aaeadeb17cd0087f922": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4885ed417c04110838ef7450afceff3",
              "IPY_MODEL_e61872d0b4e24116b6d78741d8618b0b",
              "IPY_MODEL_88d6859d500641fa9d8aab03562e1d48"
            ],
            "layout": "IPY_MODEL_3d2a38830c6a4e448ee8990763ae1af7"
          }
        },
        "9893409049e94e06900d27a726a6d560": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0d0305c516346f0a1fe0c5bad66ad66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4885ed417c04110838ef7450afceff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9893409049e94e06900d27a726a6d560",
            "placeholder": "​",
            "style": "IPY_MODEL_7fd031807d344d45a204e0950f0f64f4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e61872d0b4e24116b6d78741d8618b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2238096b40e64d7c9ecea8d66cf874b2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bd74585b8c641aa98d3630156470512",
            "value": 2
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
