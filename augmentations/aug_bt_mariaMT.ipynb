{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "\n",
        "def load_json(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def save_json(obj, path):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "\n",
        "def get_max_numeric_id(*dicts):\n",
        "    max_id = -1\n",
        "    for d in dicts:\n",
        "        if d is None:\n",
        "            continue\n",
        "        for sid in d.keys():\n",
        "            try:\n",
        "                n = int(str(sid))\n",
        "                if n > max_id:\n",
        "                    max_id = n\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return max_id if max_id >= 0 else 0\n",
        "\n",
        "\n",
        "def build_story(sample):\n",
        "    parts = [\n",
        "        sample.get(\"precontext\", \"\").strip(),\n",
        "        sample.get(\"sentence\", \"\").strip(),\n",
        "        sample.get(\"ending\", \"\").strip(),\n",
        "    ]\n",
        "    return \" \".join(p for p in parts if p)\n",
        "\n",
        "\n",
        "def build_sense_text(sample):\n",
        "    parts = [\n",
        "        sample.get(\"judged_meaning\", \"\").strip(),\n",
        "        sample.get(\"example_sentence\", \"\").strip(),\n",
        "    ]\n",
        "    return \" \".join(p for p in parts if p)\n",
        "\n",
        "\n",
        "class MarianTranslator:\n",
        "    def __init__(self, model_name, device=None):\n",
        "        self.tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "        self.model = MarianMTModel.from_pretrained(model_name)\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def translate_batch(self, texts, max_length=256, batch_size=16, desc=\"\"):\n",
        "        out = []\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=desc):\n",
        "            batch = texts[i:i + batch_size]\n",
        "            if not batch:\n",
        "                continue\n",
        "            enc = self.tokenizer(\n",
        "                batch,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "            ).to(self.device)\n",
        "            gen = self.model.generate(\n",
        "                **enc,\n",
        "                max_length=max_length,\n",
        "                num_beams=4,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            dec = self.tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "            out.extend(dec)\n",
        "        while len(out) < len(texts):\n",
        "            out.append(\"\")\n",
        "        return out[:len(texts)]\n",
        "\n",
        "\n",
        "class HFBackTranslator:\n",
        "    \"\"\"\n",
        "    en -> lang -> en using Helsinki-NLP Marian models.\n",
        "    \"\"\"\n",
        "    def __init__(self, lang_code, device=None):\n",
        "        self.lang_code = lang_code\n",
        "        en2lang = f\"Helsinki-NLP/opus-mt-en-{lang_code}\"\n",
        "        lang2en = f\"Helsinki-NLP/opus-mt-{lang_code}-en\"\n",
        "        self.en_to_lang = MarianTranslator(en2lang, device=device)\n",
        "        self.lang_to_en = MarianTranslator(lang2en, device=device)\n",
        "\n",
        "    def back_translate_list(self, texts, max_length=256, batch_size=16):\n",
        "        mid = self.en_to_lang.translate_batch(\n",
        "            texts,\n",
        "            max_length=max_length,\n",
        "            batch_size=batch_size,\n",
        "            desc=f\"BT en->{self.lang_code}\",\n",
        "        )\n",
        "        back = self.lang_to_en.translate_batch(\n",
        "            mid,\n",
        "            max_length=max_length,\n",
        "            batch_size=batch_size,\n",
        "            desc=f\"BT {self.lang_code}->en\",\n",
        "        )\n",
        "        return back\n",
        "\n",
        "\n",
        "class MPNetHelper:\n",
        "    def __init__(self, train_dict, device=None, batch_size=64):\n",
        "        self.train = train_dict\n",
        "        self.ids = list(train_dict.keys())\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = SentenceTransformer(\n",
        "            \"sentence-transformers/all-mpnet-base-v2\",\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        print(\"Encoding train stories with all-mpnet-base-v2...\")\n",
        "        self.story_texts = [build_story(train_dict[sid]) for sid in self.ids]\n",
        "        self.story_embeddings = self.model.encode(\n",
        "            self.story_texts,\n",
        "            batch_size=batch_size,\n",
        "            convert_to_tensor=True,\n",
        "            show_progress_bar=True,\n",
        "        )\n",
        "\n",
        "        print(\"Encoding train senses with all-mpnet-base-v2...\")\n",
        "        self.sense_texts = [build_sense_text(train_dict[sid]) for sid in self.ids]\n",
        "        self.sense_embeddings = self.model.encode(\n",
        "            self.sense_texts,\n",
        "            batch_size=batch_size,\n",
        "            convert_to_tensor=True,\n",
        "            show_progress_bar=True,\n",
        "        )\n",
        "\n",
        "        self.id_to_idx = {sid: i for i, sid in enumerate(self.ids)}\n",
        "\n",
        "    def story_neighbors(self, sid, top_k=10):\n",
        "        idx = self.id_to_idx[sid]\n",
        "        q = self.story_embeddings[idx].unsqueeze(0)\n",
        "        hits = util.semantic_search(q, self.story_embeddings, top_k=top_k + 1)[0]\n",
        "        out = []\n",
        "        for h in hits:\n",
        "            other_id = self.ids[h[\"corpus_id\"]]\n",
        "            if other_id == sid:\n",
        "                continue\n",
        "            out.append((other_id, float(h[\"score\"])))\n",
        "        return out\n",
        "\n",
        "    def sense_neighbors_for_vector(self, sense_vec, top_k=10):\n",
        "        hits = util.semantic_search(\n",
        "            sense_vec.unsqueeze(0),\n",
        "            self.sense_embeddings,\n",
        "            top_k=top_k,\n",
        "        )[0]\n",
        "        out = []\n",
        "        for h in hits:\n",
        "            other_id = self.ids[h[\"corpus_id\"]]\n",
        "            out.append((other_id, float(h[\"score\"])))\n",
        "        return out\n",
        "\n",
        "\n",
        "class T5Helper:\n",
        "    def __init__(self, model_name=\"t5-base\", device=None):\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_batch(\n",
        "        self,\n",
        "        prompts,\n",
        "        max_length=80,\n",
        "        num_beams=4,\n",
        "        do_sample=True,\n",
        "        temperature=1.0,\n",
        "        top_p=0.9,\n",
        "        batch_size=8,\n",
        "        desc=\"T5 generation\",\n",
        "    ):\n",
        "        out = []\n",
        "        for i in tqdm(range(0, len(prompts), batch_size), desc=desc):\n",
        "            batch = prompts[i:i + batch_size]\n",
        "            if not batch:\n",
        "                continue\n",
        "            enc = self.tokenizer(\n",
        "                batch,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=128,\n",
        "            ).to(self.device)\n",
        "            gen = self.model.generate(\n",
        "                **enc,\n",
        "                max_length=max_length,\n",
        "                num_beams=num_beams,\n",
        "                do_sample=do_sample,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            dec = self.tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "            out.extend(dec)\n",
        "        while len(out) < len(prompts):\n",
        "            out.append(\"\")\n",
        "        return out[:len(prompts)]\n",
        "\n",
        "\n",
        "class AugmentationPipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_data,\n",
        "        dev_data=None,\n",
        "        device=None,\n",
        "        seed=42,\n",
        "        enable_t5=True,\n",
        "        bt_langs=(\"de\", \"fr\"),\n",
        "        bt_batch_size=16,\n",
        "        bt_max_length=256,\n",
        "    ):\n",
        "        self.train = train_data\n",
        "        self.dev = dev_data or {}\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        random.seed(seed)\n",
        "\n",
        "        self.combined = dict(train_data)\n",
        "        self.next_id = get_max_numeric_id(train_data, dev_data) + 1\n",
        "\n",
        "        self.mpnet = MPNetHelper(self.train, device=self.device)\n",
        "        self.t5 = T5Helper(device=self.device) if enable_t5 else None\n",
        "\n",
        "        self.bt_langs = bt_langs\n",
        "        self.bt_batch_size = bt_batch_size\n",
        "        self.bt_max_length = bt_max_length\n",
        "\n",
        "        self.train_homs = {\n",
        "            s.get(\"homonym\", \"\").strip() for s in self.train.values()\n",
        "        }\n",
        "\n",
        "    def _add_sample(self, sample):\n",
        "        sid = str(self.next_id)\n",
        "        self.combined[sid] = sample\n",
        "        self.next_id += 1\n",
        "        return sid\n",
        "\n",
        "\n",
        "    def augment_backtranslation(self):\n",
        "        print(\"\\n=== Back-translation (DE / FR) ===\")\n",
        "        orig_ids = list(self.train.keys())\n",
        "        precontexts = [self.train[sid].get(\"precontext\", \"\") for sid in orig_ids]\n",
        "        endings = [self.train[sid].get(\"ending\", \"\") for sid in orig_ids]\n",
        "\n",
        "        for lang in self.bt_langs:\n",
        "            bt_engine = HFBackTranslator(lang, device=self.device)\n",
        "\n",
        "            bt_pre = bt_engine.back_translate_list(\n",
        "                precontexts,\n",
        "                max_length=self.bt_max_length,\n",
        "                batch_size=self.bt_batch_size,\n",
        "            )\n",
        "            bt_end = bt_engine.back_translate_list(\n",
        "                endings,\n",
        "                max_length=self.bt_max_length,\n",
        "                batch_size=self.bt_batch_size,\n",
        "            )\n",
        "\n",
        "            for i, sid in enumerate(orig_ids):\n",
        "                orig = self.train[sid]\n",
        "                new_sample = deepcopy(orig)\n",
        "                new_sample[\"precontext\"] = bt_pre[i]\n",
        "                if endings[i].strip():\n",
        "                    new_sample[\"ending\"] = bt_end[i]\n",
        "                else:\n",
        "                    new_sample[\"ending\"] = \"\"\n",
        "\n",
        "                new_sample[\"augment_type\"] = f\"bt_{lang}\"\n",
        "                new_sample[\"bt_lang\"] = lang\n",
        "                new_sample[\"parent_id\"] = sid\n",
        "                self._add_sample(new_sample)\n",
        "\n",
        "    def augment_cross_homonym_swap(\n",
        "        self,\n",
        "        top_k_neighbors=10,\n",
        "        min_story_sim=0.7,\n",
        "        keep_prob=0.5,\n",
        "    ):\n",
        "        print(\"\\n=== Cross-homonym swap (different homonyms, mixed context) ===\")\n",
        "        for sid, A in tqdm(self.train.items()):\n",
        "            if random.random() > keep_prob:\n",
        "                continue\n",
        "            hom_A = A.get(\"homonym\", \"\").strip()\n",
        "            if not hom_A:\n",
        "                continue\n",
        "\n",
        "            neighbors = self.mpnet.story_neighbors(sid, top_k=top_k_neighbors)\n",
        "            chosen = None\n",
        "            for nb_sid, sim in neighbors:\n",
        "                B = self.train[nb_sid]\n",
        "                hom_B = B.get(\"homonym\", \"\").strip()\n",
        "                if not hom_B or hom_B == hom_A:\n",
        "                    continue\n",
        "                if sim < min_story_sim:\n",
        "                    continue\n",
        "                chosen = (nb_sid, sim)\n",
        "                break\n",
        "\n",
        "            if not chosen:\n",
        "                continue\n",
        "\n",
        "            nb_sid, sim = chosen\n",
        "            B = self.train[nb_sid]\n",
        "\n",
        "            new_sample = deepcopy(A)\n",
        "            new_sample[\"precontext\"] = B.get(\"precontext\", \"\")\n",
        "\n",
        "            new_sample[\"augment_type\"] = \"cross_homonym_swap\"\n",
        "            new_sample[\"parent_id\"] = sid\n",
        "            new_sample[\"context_from_id\"] = nb_sid\n",
        "            new_sample[\"story_sim_score\"] = sim\n",
        "\n",
        "            self._add_sample(new_sample)\n",
        "\n",
        "    def augment_context_variation(\n",
        "        self,\n",
        "        keep_prob=0.5,\n",
        "        max_len_ctx=80,\n",
        "        max_len_sent=40,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Change up precontext and ending via paraphrasing, and optionally\n",
        "        the sentence, while keeping the homonym unchanged.\n",
        "        \"\"\"\n",
        "        if self.t5 is None:\n",
        "            print(\"\\n=== Context variation: T5 disabled, skipping ===\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== Context variation (T5 paraphrase of precontext / ending / sentence) ===\")\n",
        "        candidates = [sid for sid in self.train.keys() if random.random() < keep_prob]\n",
        "\n",
        "        pre_prompts = []\n",
        "        end_prompts = []\n",
        "        sent_prompts = []\n",
        "        for sid in candidates:\n",
        "            s = self.train[sid]\n",
        "            pre = s.get(\"precontext\", \"\").strip()\n",
        "            end = s.get(\"ending\", \"\").strip()\n",
        "            hom = s.get(\"homonym\", \"\").strip()\n",
        "            sent = s.get(\"sentence\", \"\").strip()\n",
        "\n",
        "            pre_prompts.append(f\"Paraphrase this context in English:\\n{pre}\" if pre else \"\")\n",
        "            end_prompts.append(f\"Paraphrase this ending in English:\\n{end}\" if end else \"\")\n",
        "\n",
        "            if hom and sent:\n",
        "                sent_prompts.append(\n",
        "                    f\"Paraphrase the following sentence while keeping the word '{hom}' unchanged:\\n{sent}\"\n",
        "                )\n",
        "            else:\n",
        "                sent_prompts.append(\"\")\n",
        "\n",
        "        pre_out = self.t5.generate_batch(\n",
        "            pre_prompts,\n",
        "            max_length=max_len_ctx,\n",
        "            desc=\"T5 paraphrase precontext\",\n",
        "        )\n",
        "        end_out = self.t5.generate_batch(\n",
        "            end_prompts,\n",
        "            max_length=max_len_ctx,\n",
        "            desc=\"T5 paraphrase ending\",\n",
        "        )\n",
        "        sent_out = self.t5.generate_batch(\n",
        "            sent_prompts,\n",
        "            max_length=max_len_sent,\n",
        "            desc=\"T5 paraphrase sentence\",\n",
        "        )\n",
        "\n",
        "        for sid, new_pre, new_end, new_sent in zip(candidates, pre_out, end_out, sent_out):\n",
        "            orig = self.train[sid]\n",
        "            hom = orig.get(\"homonym\", \"\").strip()\n",
        "\n",
        "            new_sample = deepcopy(orig)\n",
        "\n",
        "            if orig.get(\"precontext\", \"\").strip() and new_pre.strip():\n",
        "                new_sample[\"precontext\"] = new_pre.strip()\n",
        "\n",
        "            if orig.get(\"ending\", \"\").strip() and new_end.strip():\n",
        "                new_sample[\"ending\"] = new_end.strip()\n",
        "\n",
        "            new_sent = new_sent.strip()\n",
        "            if hom and new_sent and hom.lower() in new_sent.lower():\n",
        "                new_sample[\"sentence\"] = new_sent\n",
        "\n",
        "            new_sample[\"augment_type\"] = \"context_variation_t5\"\n",
        "            new_sample[\"parent_id\"] = sid\n",
        "            new_sample[\"t5_model\"] = \"t5-base\"\n",
        "\n",
        "            if (\n",
        "                new_sample[\"precontext\"] == orig.get(\"precontext\", \"\") and\n",
        "                new_sample[\"ending\"] == orig.get(\"ending\", \"\") and\n",
        "                new_sample[\"sentence\"] == orig.get(\"sentence\", \"\")\n",
        "            ):\n",
        "                continue\n",
        "\n",
        "            self._add_sample(new_sample)\n",
        "\n",
        "\n",
        "    def augment_synthetic_dev_homonyms(\n",
        "        self,\n",
        "        keep_prob=1.0,\n",
        "        max_len=80,\n",
        "        top_k_label_neighbors=5,\n",
        "    ):\n",
        "        if self.t5 is None:\n",
        "            print(\"\\n=== Synthetic dev homonyms: T5 disabled, skipping ===\")\n",
        "            return\n",
        "        if not self.dev:\n",
        "            print(\"\\n=== Synthetic dev homonyms: no dev provided, skipping ===\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== Synthetic dev homonyms (dev-only homonyms, T5-generated stories) ===\")\n",
        "\n",
        "        dev_only_ids = []\n",
        "        for did, d in self.dev.items():\n",
        "            hom = d.get(\"homonym\", \"\").strip()\n",
        "            if hom and hom not in self.train_homs and random.random() < keep_prob:\n",
        "                dev_only_ids.append(did)\n",
        "\n",
        "        if not dev_only_ids:\n",
        "            print(\"No dev-only homonyms found or none selected by keep_prob.\")\n",
        "            return\n",
        "\n",
        "        prompts = []\n",
        "        dev_sense_texts = []\n",
        "        for did in dev_only_ids:\n",
        "            d = self.dev[did]\n",
        "            hom = d.get(\"homonym\", \"\").strip()\n",
        "            meaning = d.get(\"judged_meaning\", \"\").strip()\n",
        "            if not hom or not meaning:\n",
        "                prompts.append(\"\")\n",
        "                dev_sense_texts.append(\"\")\n",
        "                continue\n",
        "\n",
        "            prompt = (\n",
        "                \"Write three lines.\\n\"\n",
        "                \"Line 1: a short precontext before a sentence.\\n\"\n",
        "                f\"Line 2: a sentence that contains the word '{hom}' used in the sense '{meaning}'.\\n\"\n",
        "                \"Line 3: an ending that continues the story.\"\n",
        "            )\n",
        "            prompts.append(prompt)\n",
        "            dev_sense_texts.append(build_sense_text(d))\n",
        "\n",
        "        gen = self.t5.generate_batch(\n",
        "            prompts,\n",
        "            max_length=max_len,\n",
        "            desc=\"T5 synthetic dev homonym stories\",\n",
        "        )\n",
        "\n",
        "        dev_sense_embs = []\n",
        "        for txt in dev_sense_texts:\n",
        "            if txt.strip():\n",
        "                emb = self.mpnet.model.encode(txt, convert_to_tensor=True)\n",
        "            else:\n",
        "                emb = None\n",
        "            dev_sense_embs.append(emb)\n",
        "\n",
        "        for did, out_text, sense_emb in zip(dev_only_ids, gen, dev_sense_embs):\n",
        "            d = self.dev[did]\n",
        "            hom = d.get(\"homonym\", \"\").strip()\n",
        "            meaning = d.get(\"judged_meaning\", \"\").strip()\n",
        "            ex_sent = d.get(\"example_sentence\", \"\").strip()\n",
        "\n",
        "            if not hom or not meaning:\n",
        "                continue\n",
        "\n",
        "            lines = [ln.strip() for ln in out_text.split(\"\\n\") if ln.strip()]\n",
        "            if len(lines) < 3:\n",
        "                if not lines:\n",
        "                    continue\n",
        "                pre = lines[0]\n",
        "                if len(lines) > 1:\n",
        "                    sent = lines[1]\n",
        "                    end = \" \".join(lines[2:]) if len(lines) > 2 else \"\"\n",
        "                else:\n",
        "                    sent, end = \"\", \"\"\n",
        "            else:\n",
        "                pre, sent, end = lines[0], lines[1], lines[2]\n",
        "\n",
        "            if not sent or (hom.lower() not in sent.lower()):\n",
        "                continue\n",
        "\n",
        "            new_sample = {\n",
        "                \"homonym\": hom,\n",
        "                \"judged_meaning\": meaning,\n",
        "                \"precontext\": pre,\n",
        "                \"sentence\": sent,\n",
        "                \"ending\": end,\n",
        "                \"example_sentence\": ex_sent,\n",
        "            }\n",
        "\n",
        "            if sense_emb is not None:\n",
        "                neighbors = self.mpnet.sense_neighbors_for_vector(\n",
        "                    sense_emb,\n",
        "                    top_k=top_k_label_neighbors,\n",
        "                )\n",
        "                if neighbors:\n",
        "                    label_id, sim = neighbors[0]\n",
        "                    donor = self.train[label_id]\n",
        "                    for fld in [\"choices\", \"average\", \"stdev\", \"nonsensical\"]:\n",
        "                        if fld in donor:\n",
        "                            new_sample[fld] = deepcopy(donor[fld])\n",
        "                    new_sample[\"label_from_id\"] = label_id\n",
        "                    new_sample[\"label_from_sim\"] = sim\n",
        "\n",
        "            new_sample[\"augment_type\"] = \"synthetic_dev_homonym_t5\"\n",
        "            new_sample[\"parent_id\"] = f\"dev_{did}\"\n",
        "            new_sample[\"t5_model\"] = \"t5-base\"\n",
        "\n",
        "            self._add_sample(new_sample)\n",
        "\n",
        "\n",
        "    def augment_rating_preserving(\n",
        "        self,\n",
        "        top_k_neighbors=10,\n",
        "        min_story_sim=0.8,\n",
        "        max_rating_diff=0.3,\n",
        "        max_per_sample=1,\n",
        "    ):\n",
        "        print(\"\\n=== Rating-preserving (neighbor context tweak) ===\")\n",
        "        for sid, A in tqdm(self.train.items()):\n",
        "            try:\n",
        "                rating_a = float(A.get(\"average\", 0.0))\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            neighbors = self.mpnet.story_neighbors(sid, top_k=top_k_neighbors)\n",
        "            created = 0\n",
        "            for nb_sid, sim in neighbors:\n",
        "                if created >= max_per_sample:\n",
        "                    break\n",
        "                B = self.train[nb_sid]\n",
        "                try:\n",
        "                    rating_b = float(B.get(\"average\", 0.0))\n",
        "                except Exception:\n",
        "                    continue\n",
        "                if abs(rating_b - rating_a) > max_rating_diff:\n",
        "                    continue\n",
        "                if sim < min_story_sim:\n",
        "                    continue\n",
        "\n",
        "                new_sample = deepcopy(A)\n",
        "                new_sample[\"precontext\"] = B.get(\"precontext\", \"\")\n",
        "                new_sample[\"average\"] = float((rating_a + rating_b) / 2.0)\n",
        "                new_sample[\"augment_type\"] = \"rating_preserving\"\n",
        "                new_sample[\"parent_id\"] = sid\n",
        "                new_sample[\"rating_neighbor_id\"] = nb_sid\n",
        "                new_sample[\"story_sim_score\"] = sim\n",
        "\n",
        "                self._add_sample(new_sample)\n",
        "                created += 1\n",
        "\n",
        "    def augment_same_context_different_homonym(\n",
        "        self,\n",
        "        top_k_neighbors=10,\n",
        "        min_story_sim=0.7,\n",
        "        max_per_sample=1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Same context shell (precontext + ending), different homonym + sentence.\n",
        "\n",
        "        For each sample A:\n",
        "            - find neighbor B with different homonym, high similarity.\n",
        "            - reuse A.precontext + A.ending,\n",
        "              but copy B.sentence, homonym, judged_meaning, labels.\n",
        "        \"\"\"\n",
        "        print(\"\\n=== Same context, different homonym ===\")\n",
        "        for sid, A in tqdm(self.train.items()):\n",
        "            hom_A = A.get(\"homonym\", \"\").strip()\n",
        "            if not hom_A:\n",
        "                continue\n",
        "\n",
        "            neighbors = self.mpnet.story_neighbors(sid, top_k=top_k_neighbors)\n",
        "            created = 0\n",
        "            for nb_sid, sim in neighbors:\n",
        "                if created >= max_per_sample:\n",
        "                    break\n",
        "                B = self.train[nb_sid]\n",
        "                hom_B = B.get(\"homonym\", \"\").strip()\n",
        "                if not hom_B or hom_B == hom_A:\n",
        "                    continue\n",
        "                if sim < min_story_sim:\n",
        "                    continue\n",
        "\n",
        "                new_sample = deepcopy(B)\n",
        "                new_sample[\"precontext\"] = A.get(\"precontext\", \"\")\n",
        "                new_sample[\"ending\"] = A.get(\"ending\", \"\")\n",
        "\n",
        "                new_sample[\"augment_type\"] = \"same_context_different_homonym\"\n",
        "                new_sample[\"parent_id\"] = nb_sid\n",
        "                new_sample[\"context_from_id\"] = sid\n",
        "                new_sample[\"story_sim_score\"] = sim\n",
        "\n",
        "                self._add_sample(new_sample)\n",
        "                created += 1\n",
        "\n",
        "\n",
        "    def run_all(self):\n",
        "        self.augment_backtranslation()\n",
        "        self.augment_cross_homonym_swap()\n",
        "        self.augment_context_variation()\n",
        "        self.augment_synthetic_dev_homonyms()\n",
        "        self.augment_rating_preserving()\n",
        "        self.augment_same_context_different_homonym()\n",
        "        return self.combined\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(\n",
        "        description=\"AmbiStory augmentation (MPNet + T5 + dev-based synthetic homonyms).\"\n",
        "    )\n",
        "    p.add_argument(\"--train_path\", type=str, default=\"/kaggle/input/ambistory-raw/train.json\")\n",
        "    p.add_argument(\"--dev_path\", type=str, default=\"/kaggle/input/ambistory-raw/dev.json\")\n",
        "    p.add_argument(\"--output_path\", type=str, default=\"train_augmented_all.json\")\n",
        "    p.add_argument(\"--device\", type=str, default=None)\n",
        "    p.add_argument(\"--disable_t5\", action=\"store_true\")\n",
        "    args, _ = p.parse_known_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    print(f\"Loading train from {args.train_path} ...\")\n",
        "    train_data = load_json(args.train_path)\n",
        "    print(f\"Loading dev from {args.dev_path} ...\")\n",
        "    dev_data = load_json(args.dev_path)\n",
        "\n",
        "    pipeline = AugmentationPipeline(\n",
        "        train_data=train_data,\n",
        "        dev_data=dev_data,\n",
        "        device=args.device,\n",
        "        enable_t5=not args.disable_t5,\n",
        "    )\n",
        "    combined = pipeline.run_all()\n",
        "\n",
        "    print(f\"\\nFinal dataset size: {len(combined)} \"\n",
        "          f\"(original: {len(train_data)}, new: {len(combined) - len(train_data)})\")\n",
        "    print(f\"Saving to {args.output_path} ...\")\n",
        "    save_json(combined, args.output_path)\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "def run_pipeline(\n",
        "    train_path,\n",
        "    dev_path,\n",
        "    output_path,\n",
        "    device=None,\n",
        "    enable_t5=True,\n",
        "):\n",
        "    print(f\"Loading train from {train_path} ...\")\n",
        "    train_data = load_json(train_path)\n",
        "    print(f\"Loading dev from {dev_path} ...\")\n",
        "    dev_data = load_json(dev_path)\n",
        "\n",
        "    pipeline = AugmentationPipeline(\n",
        "        train_data=train_data,\n",
        "        dev_data=dev_data,\n",
        "        device=device,\n",
        "        enable_t5=enable_t5,\n",
        "    )\n",
        "    combined = pipeline.run_all()\n",
        "\n",
        "    print(f\"\\nFinal dataset size: {len(combined)} \"\n",
        "          f\"(original: {len(train_data)}, new: {len(combined) - len(train_data)})\")\n",
        "    print(f\"Saving to {output_path} ...\")\n",
        "    save_json(combined, output_path)\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2238096b40e64d7c9ecea8d66cf874b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d2a38830c6a4e448ee8990763ae1af7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f17cb8b0c2d48bbbad94cae58ee6eab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fd031807d344d45a204e0950f0f64f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88d6859d500641fa9d8aab03562e1d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f17cb8b0c2d48bbbad94cae58ee6eab",
            "placeholder": "​",
            "style": "IPY_MODEL_b0d0305c516346f0a1fe0c5bad66ad66",
            "value": " 2/2 [00:03&lt;00:00,  1.45s/it]"
          }
        },
        "8bd74585b8c641aa98d3630156470512": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d48e1c080474aaeadeb17cd0087f922": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4885ed417c04110838ef7450afceff3",
              "IPY_MODEL_e61872d0b4e24116b6d78741d8618b0b",
              "IPY_MODEL_88d6859d500641fa9d8aab03562e1d48"
            ],
            "layout": "IPY_MODEL_3d2a38830c6a4e448ee8990763ae1af7"
          }
        },
        "9893409049e94e06900d27a726a6d560": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0d0305c516346f0a1fe0c5bad66ad66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4885ed417c04110838ef7450afceff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9893409049e94e06900d27a726a6d560",
            "placeholder": "​",
            "style": "IPY_MODEL_7fd031807d344d45a204e0950f0f64f4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e61872d0b4e24116b6d78741d8618b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2238096b40e64d7c9ecea8d66cf874b2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bd74585b8c641aa98d3630156470512",
            "value": 2
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
