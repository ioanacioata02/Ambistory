{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b9d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nlpaug\n",
    "def worker_init_fn(worker_id):\n",
    "    import nltk\n",
    "    for pkg in [\"averaged_perceptron_tagger\", \"wordnet\", \"omw-1.4\"]:\n",
    "        try:\n",
    "            nltk.data.find(pkg)\n",
    "        except LookupError:\n",
    "            nltk.download(pkg, quiet=True)\n",
    "    nltk.download = lambda *args, **kwargs: True\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f706dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import nlpaug.augmenter.word as naw\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b651dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentManager:\n",
    "    @staticmethod\n",
    "    def set_seed(seed=42):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac852eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreCalculator:\n",
    "    def __init__(self, beta):\n",
    "        self.beta = beta\n",
    "\n",
    "    def get_weighted_score(self, average):\n",
    "        vals, counts = np.unique(average, return_counts=True)\n",
    "        weights = np.exp(counts * self.beta)\n",
    "        weighted_avg = np.sum(vals * weights) / np.sum(weights)\n",
    "        \n",
    "        return weighted_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics\n",
    "class AmbiStoryDataset(Dataset):\n",
    "    def __init__(self, orig_path, tokenizer, config, bt_path=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cfg = config\n",
    "        self.score_engine = ScoreCalculator(config['beta'])\n",
    "        \n",
    "        with open(orig_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.sids = list(self.data.keys())\n",
    "        \n",
    "        self.bt_data = None\n",
    "        if bt_path:\n",
    "            with open(bt_path, 'r') as f:\n",
    "                self.bt_data = json.load(f)\n",
    "            self._init_augmenters()\n",
    "\n",
    "    def _init_augmenters(self):\n",
    "        p = self.cfg.get('eda_intensity', 0.1)\n",
    "        \n",
    "        self.syn_aug = naw.SynonymAug(aug_src='wordnet', aug_p=p)\n",
    "        self.del_aug = naw.RandomWordAug(action=\"delete\", aug_p=p)\n",
    "        self.swap_aug = naw.RandomWordAug(action=\"swap\", aug_p=p)\n",
    "\n",
    "    def _apply_eda(self, text):\n",
    "        if not text or len(text.split()) < 5: \n",
    "            return text\n",
    "            \n",
    "        mode = self.cfg.get('eda_mode', 'random')\n",
    "        \n",
    "        if mode == 'random':\n",
    "            eda_cfg = self.cfg.get('eda_weights', {\"synonym\": 1, \"deletion\": 1, \"swap\": 1})\n",
    "            techniques = ['synonym', 'deletion', 'swap']\n",
    "            weights = [eda_cfg['synonym'], eda_cfg['deletion'], eda_cfg['swap']]\n",
    "            \n",
    "            choice = random.choices(techniques, weights=weights, k=1)[0]\n",
    "            \n",
    "            try:\n",
    "                if choice == 'synonym': return self.syn_aug.augment(text)[0]\n",
    "                elif choice == 'deletion': return self.del_aug.augment(text)[0]\n",
    "                elif choice == 'swap': return self.swap_aug.augment(text)[0]\n",
    "            except: return text\n",
    "\n",
    "        elif mode == 'chain':\n",
    "            augmented_text = text\n",
    "            try:\n",
    "                augmented_text = self.syn_aug.augment(augmented_text)[0]\n",
    "                augmented_text = self.swap_aug.augment(augmented_text)[0]\n",
    "                augmented_text = self.del_aug.augment(augmented_text)[0]\n",
    "                return augmented_text\n",
    "            except:\n",
    "                return text\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = self.sids[idx]\n",
    "        item_orig = self.data[sid]\n",
    "    \n",
    "        if self.bt_data and random.random() < self.cfg['p_use_bt']:\n",
    "            source = self.bt_data[sid]\n",
    "        else:\n",
    "            source = item_orig\n",
    "    \n",
    "        precontext = source['precontext']\n",
    "        ending = source['ending']\n",
    "        \n",
    "\n",
    "        if self.bt_data and random.random() < self.cfg['p_eda']:\n",
    "            precontext = self._apply_eda(precontext)\n",
    "            ending = self._apply_eda(ending)\n",
    "    \n",
    "        \n",
    "        full_story = f\"{precontext} {item_orig['sentence']} {ending}\".strip()\n",
    "        full_meaning = f\"{item_orig['example_sentence']} {item_orig['judged_meaning']}\".strip()\n",
    "        encoding = self.tokenizer(\n",
    "                full_story,\n",
    "                item_orig['judged_meaning'],\n",
    "                max_length=self.cfg['max_length'],\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "        ratings = item_orig[\"choices\"]\n",
    "        gold_mean = float(np.mean(ratings))\n",
    "        gold_stdev = float(statistics.stdev(ratings)) if len(ratings) >= 2 else 0.0\n",
    "\n",
    "        if self.bt_data:\n",
    "            label_scalar = float(self.score_engine.get_weighted_score(ratings))\n",
    "        else:\n",
    "            label_scalar = gold_mean\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label_scalar, dtype=torch.float),\n",
    "            'gold_mean': torch.tensor(gold_mean, dtype=torch.float),\n",
    "            'gold_stdev': torch.tensor(gold_stdev, dtype=torch.float),\n",
    "            'id': int(sid) if str(sid).isdigit() else sid,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9998389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "@torch.no_grad()\n",
    "def official_scores_from_stats(model, loader, device, clamp_1_5=True, rounded=False):\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    gold_means = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        gold_mean = batch[\"gold_mean\"].cpu().numpy().astype(float)\n",
    "        gold_stdev = batch[\"gold_stdev\"].cpu().numpy().astype(float)\n",
    "\n",
    "        out = model(ids, attention_mask=mask)\n",
    "        p = out.logits.squeeze(-1).detach().cpu().numpy().astype(float)\n",
    "\n",
    "        if clamp_1_5:\n",
    "            p = np.clip(p, 1.0, 5.0)\n",
    "        if rounded:\n",
    "            p = np.rint(p).astype(int)\n",
    "\n",
    "        preds.extend(p.tolist())\n",
    "        gold_means.extend(gold_mean.tolist())\n",
    "\n",
    "        for pred, m, sd in zip(p, gold_mean, gold_stdev):\n",
    "            ok = ((m - sd) < pred < (m + sd)) or (abs(m - pred) < 1.0)\n",
    "            correct += int(ok)\n",
    "            total += 1\n",
    "\n",
    "    corr, _ = spearmanr(preds, gold_means)\n",
    "    acc = correct / total if total else 0.0\n",
    "    return float(corr), float(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68219c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.cfg = config\n",
    "        self.device = torch.device(config['device'])\n",
    "        self._prepare_components()\n",
    "        self.best_dev_acc = -1.0\n",
    "        self.best_epoch = -1\n",
    "\n",
    "    def _prepare_components(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.cfg['model_name'])\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.cfg['model_name'], num_labels=1\n",
    "        ).to(self.device)\n",
    "            \n",
    "        train_ds = AmbiStoryDataset(self.cfg['train_orig'], self.tokenizer, self.cfg, bt_path=self.cfg['train_bt'])\n",
    "        dev_ds = AmbiStoryDataset(self.cfg['dev_orig'], self.tokenizer, self.cfg)\n",
    "        \n",
    "        self.train_loader = DataLoader(train_ds, batch_size=self.cfg['batch_size'], shuffle=True, num_workers=2,\n",
    "                                        worker_init_fn=worker_init_fn,persistent_workers=True)\n",
    "        self.dev_loader = DataLoader(dev_ds, batch_size=self.cfg['batch_size'], num_workers=2,\n",
    "                                        worker_init_fn=worker_init_fn,persistent_workers=True)\n",
    "        \n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.cfg['learning_rate'],weight_decay=self.cfg['weight_decay'])\n",
    "        \n",
    "        self.loss_fn = torch.nn.HuberLoss(delta=self.cfg['huber_delta'])\n",
    "        self.label_smoothing = self.cfg['label_smoothing']\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='max',               \n",
    "            factor=0.5,                \n",
    "            patience=3,               \n",
    "            threshold=1e-4,      \n",
    "            min_lr=1e-6\n",
    "        )\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        print(f\"Starting Training on {self.device}\")\n",
    "        for epoch in range(self.cfg['epochs']):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            tk = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "            \n",
    "            for batch in tk:\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                ids = batch['input_ids'].to(self.device)\n",
    "                mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                out = self.model(ids, attention_mask=mask)\n",
    "\n",
    "                if self.label_smoothing > 0.0:\n",
    "                    center = self.cfg['label_smoothing_center']\n",
    "                    smoothed_labels = (\n",
    "                        (1.0 - self.label_smoothing) * labels\n",
    "                        + self.label_smoothing * center\n",
    "                    )\n",
    "                else:\n",
    "                    smoothed_labels = labels\n",
    "                    \n",
    "                loss = self.loss_fn(out.logits.flatten(), smoothed_labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                tk.set_postfix(loss=total_loss/len(self.train_loader))\n",
    "                \n",
    "            train_s, train_a = official_scores_from_stats(self.model, self.train_loader, self.device)\n",
    "            dev_s, dev_a = official_scores_from_stats(self.model, self.dev_loader, self.device)\n",
    "            dev_s_round, dev_a_round = official_scores_from_stats(self.model, self.dev_loader, self.device, rounded=True)\n",
    "\n",
    "            self.scheduler.step(train_a)\n",
    "            \n",
    "            if dev_a_round > self.best_dev_acc:\n",
    "                self.best_dev_acc = dev_a_round\n",
    "                self.best_epoch = epoch + 1\n",
    "                torch.save(self.model.state_dict(), \"best_model.pt\")\n",
    "                print(f\"New best model saved (epoch {self.best_epoch}) | best_dev_acc={self.best_dev_acc:.4f}\")\n",
    "            \n",
    "                        \n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{self.cfg['epochs']} | \",\n",
    "                f\"TRAIN Spearman={train_s:.4f} Acc@SD/1={train_a:.4f} | \",\n",
    "                f\"DEV Spearman={dev_s:.4f} Acc@SD/1={dev_a:.4f} | \",\n",
    "                f\"DEV_ROUND Spearman={dev_s_round:.4f} Acc@SD/1={dev_a_round:.4f}\",\n",
    "                flush=True\n",
    "            )\n",
    "            \n",
    "        if os.path.exists(\"best_model.pt\"):\n",
    "            self.model.load_state_dict(torch.load(\"best_model.pt\", map_location=self.device))\n",
    "            print(f\"Loaded best model from best_model.pt (epoch {self.best_epoch}, best_dev_acc={self.best_dev_acc:.4f})\")\n",
    "        else:\n",
    "            print(\"Best model file not found â€” using current model weights.\")\n",
    "\n",
    "        save_predictions_jsonl(\n",
    "            model=self.model,\n",
    "            loader=self.dev_loader,\n",
    "            out_path=f\"predictions.jsonl\",\n",
    "            device=self.device,\n",
    "            round_to_int=True\n",
    "        )\n",
    "import json\n",
    "import torch\n",
    "\n",
    "def save_predictions_jsonl(model, loader, out_path, device, round_to_int=True):\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            ids = batch[\"id\"] \n",
    "            if torch.is_tensor(ids):\n",
    "                ids = ids.cpu().tolist()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            logits = outputs.logits.squeeze(-1)\n",
    "\n",
    "            pred_vals = logits.detach().cpu().tolist()\n",
    "            if not isinstance(pred_vals, list):\n",
    "                pred_vals = [pred_vals]\n",
    "\n",
    "            for sid, p in zip(ids, pred_vals):\n",
    "                if round_to_int:\n",
    "                    p = int(round(float(p)))\n",
    "                    p = max(1, min(5, p))\n",
    "                else:\n",
    "                    p = float(p)\n",
    "\n",
    "                preds.append({\"id\": str(sid), \"prediction\": p})\n",
    "\n",
    "    def _sort_key(x):\n",
    "        try:\n",
    "            return int(x[\"id\"])\n",
    "        except:\n",
    "            return x[\"id\"]\n",
    "\n",
    "    preds.sort(key=_sort_key)\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in preds:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved {len(preds)} predictions to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    EnvironmentManager.set_seed(42)\n",
    "    \n",
    "    CONFIG = {\n",
    "        \"model_name\": \"roberta-base\",\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 30,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"max_length\": 256,\n",
    "        \"beta\": 0.5,\n",
    "        \"train_orig\": \"/kaggle/input/ambistory-raw/train.json\",\n",
    "        \"train_bt\": \"/kaggle/input/ambistory-processed/train_bt_only.json\",\n",
    "        \"dev_orig\": \"/kaggle/input/ambistory-raw/dev.json\",\n",
    "        \"p_use_bt\": 0.5,\n",
    "        \"p_eda\": 0.2,\n",
    "        \"eda_intensity\": 0.1,\n",
    "        \"eda_mode\": \"random\",\n",
    "        \"eda_weights\": {\n",
    "            \"synonym\": 0.7,\n",
    "            \"deletion\": 0.15,\n",
    "            \"swap\": 0.15\n",
    "        },\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"label_smoothing\": 0.1,\n",
    "        \"label_smoothing_center\": 3.0,\n",
    "        #\"hidden_dropout_prob\": 0.1,\n",
    "        #\"attention_dropout_prob\": 0.1,\n",
    "        #\"classifier_dropout\": 0.1,\n",
    "        #\"warmup_ratio\": 0.1,\n",
    "        \"huber_delta\": 1,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    }\n",
    "\n",
    "    orchestrator = Trainer(CONFIG)\n",
    "    orchestrator.fit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
